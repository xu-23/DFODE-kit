{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44caac4b",
   "metadata": {},
   "source": [
    "This is a tutorial for the DFODE-kit package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1a510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dfode_kit.df_interface import (\n",
    "    OneDFreelyPropagatingFlameConfig,\n",
    "    setup_one_d_flame_case,\n",
    "    df_to_h5,\n",
    ")\n",
    "from dfode_kit.data_operations import (\n",
    "    touch_h5, \n",
    "    get_TPY_from_h5, \n",
    "    random_perturb,\n",
    "    label_npy,\n",
    "    integrate_h5,\n",
    "    calculate_error,\n",
    ")\n",
    "from dfode_kit.dfode_core.model.mlp import MLP\n",
    "from dfode_kit.utils import BCT\n",
    "\n",
    "DFODE_ROOT = os.environ['DFODE_ROOT']\n",
    "\n",
    "print(DFODE_ROOT)\n",
    "\n",
    "eq_ratios = [0.8, 1.0, 1.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0032661c",
   "metadata": {},
   "source": [
    "### A brief introduction to the DFODE method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a592b7b",
   "metadata": {},
   "source": [
    "#### Low-dimensional manifold sampling\n",
    "\n",
    "A key challenge in preparing training data is achieving sufficient coverage of the relevant thermochemical composition space, which is often prohibitively high-dimensional when detailed chemistry involves tens to hundreds of species. \n",
    "\n",
    "To address this, DFODE-kit adopts a low-dimensional\n",
    "manifold sampling strategy, where thermochemical states are extracted from canonical flame configurations that retain the essential topology of high-dimensional turbulent flames. This approach ensures both computational efficiency and physical representativeness of the training datasets.\n",
    "\n",
    "In this tutorial, we will demonstrate how to use DFODE-kit to sample a low-dimensional manifold of thermochemical states from a one-dimensional laminar freely propagating flame simulated with DeepFlame. The following code block could also be found in `dfode_kit_init.ipynb` files within the case templates provides in the `canonical_cases` directory. It is used to initialize the simulation and update the dictionary files for the simulation.\n",
    "\n",
    "Note:\n",
    "For this demonstration, a number of 10 for output steps is adopted per case. This parameter can be adjusted upward in actual implementations to enhance statistical reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d8394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "standard_case = 'standard_case_for_sampling'\n",
    "\n",
    "num_output_steps = 100\n",
    "\n",
    "standard_case_path = f\"{DFODE_ROOT}/tutorials/twoD_HIT_flame/1_sample_train/{standard_case}\"\n",
    "\n",
    "for eq_ratio in eq_ratios:\n",
    "    new_case_name = f\"eq_ratio_{eq_ratio}\"\n",
    "    new_case_path = f\"{DFODE_ROOT}/tutorials/twoD_HIT_flame/1_sample_train/{new_case_name}\"\n",
    "\n",
    "    if os.path.exists(standard_case_path):\n",
    "        shutil.copytree(standard_case_path, new_case_path)\n",
    "        print(f\"copy {standard_case} to {new_case_name}\")\n",
    "\n",
    "        original_dir = os.getcwd()\n",
    "        os.chdir(new_case_path)\n",
    "        \n",
    "        try:\n",
    "            # Operating condition settings\n",
    "            config_dict = {\n",
    "                \"mechanism\": f\"{DFODE_ROOT}/mechanisms/Burke2012_s9r23.yaml\",\n",
    "                \"T0\": 500,\n",
    "                \"p0\": 101325,\n",
    "                \"fuel\": \"H2:1\",\n",
    "                \"oxidizer\": \"O2:0.21,N2:0.79\",\n",
    "                \"eq_ratio\": eq_ratio,  \n",
    "            }\n",
    "            config = OneDFreelyPropagatingFlameConfig(**config_dict)\n",
    "\n",
    "            # Simulation settings\n",
    "            settings = {\n",
    "                \"sim_time_step\": 1e-6,\n",
    "                \"sim_write_interval\": 1e-5,\n",
    "                \"num_output_steps\": num_output_steps,\n",
    "            }\n",
    "            config.update_config(settings)\n",
    "\n",
    "            # Setup the case and update dictionary files\n",
    "            setup_one_d_flame_case(config, '.')\n",
    "            \n",
    "            print(f\"successfully setting {new_case_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Wrong in setting {new_case_name}: {e}\")\n",
    "        finally:\n",
    "            os.chdir(original_dir)\n",
    "    else:\n",
    "        print(f\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbbe369",
   "metadata": {},
   "source": [
    "Note that at the point, the simulation is not yet started. The user would need to ensure a working version of DeepFlame is available and run the `Allrun` script from command line to start the simulation.\n",
    "\n",
    "```bash\n",
    "./Allrun\n",
    "```\n",
    "\n",
    "After the simulation is completed, we proceed to use DFODE-kit to gather and manage the thermochemical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd60205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eq_ratio in eq_ratios:\n",
    "    df_to_h5(\n",
    "        root_dir=f\"{DFODE_ROOT}/tutorials/twoD_HIT_flame/1_sample_train/eq_ratio_{eq_ratio}\",\n",
    "        mechanism=f\"{DFODE_ROOT}/mechanisms/Burke2012_s9r23.yaml\",\n",
    "        hdf5_file_path=f\"{DFODE_ROOT}/tutorials/twoD_HIT_flame/1_sample_train/tutorial_data_{eq_ratio}.h5\",\n",
    "        include_mesh=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779899a3",
   "metadata": {},
   "source": [
    "#### Data augmentation and labeling\n",
    "\n",
    "While laminar canonical flames provide fundamental thermochemical states,their trajectory-aligned sampling in composition space poses significant limitations for a posteriori modeling applications. First, these sampled states are confined to predefined flamelet manifolds, making the trained model highly sensitive to perturbations and leading to an over-constrained representation. Second, the sampled states span a lower-dimensional subspace, which fails to encompass the full range of thermochemical variations encountered in turbulent combustion. As a result, the model becomes vulnerable to off-manifold perturbations—deviations from the training manifold that frequently arise in turbulent reacting flows.\n",
    "\n",
    "To tackle this challenge, a data augmentation strategy is employed, where collected states are perturbed to simulate the effects of multi-dimensional transport and turbulence disturbances.\n",
    "\n",
    "Note:\n",
    "For this demonstration, a sampling size of 20,000 is adopted per case. This parameter can be adjusted upward in actual implementations to enhance statistical reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece01cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_num = 600000\n",
    "\n",
    "for eq_ratio in eq_ratios:\n",
    "    h5_file = f'tutorial_data_{eq_ratio}.h5'\n",
    "    mech = f'{DFODE_ROOT}/mechanisms/Burke2012_s9r23.yaml'\n",
    "    output_file = f'data_{eq_ratio}'\n",
    "\n",
    "    print(f\"Loading data from h5 file: {h5_file}\")\n",
    "    data = get_TPY_from_h5(h5_file)    \n",
    "    print(\"Data shape:\", data.shape)\n",
    "    All_data = random_perturb(data, mech, dataset_num, heat_limit=False, element_limit=True, eq_ratio=eq_ratio)\n",
    "    np.save(output_file, All_data)\n",
    "    print(\"Saved augmented data shape:\", All_data.shape)\n",
    "    print(f\"Saved augmented data to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729f2d8f",
   "metadata": {},
   "source": [
    "The CVODE integrator from Cantera is used for time integration and to provide supervised learning labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff47da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dfode_kit.data_operations import label_npy\n",
    "\n",
    "all_labeled_data = []\n",
    "\n",
    "for eq_ratio in eq_ratios:\n",
    "    try:\n",
    "        labeled_data = label_npy(\n",
    "            mech_path=mech,\n",
    "            time_step=1e-06,\n",
    "            source_path=f'./data_{eq_ratio}.npy'\n",
    "        )\n",
    "        all_labeled_data.append(labeled_data)\n",
    "        \n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "all_labeled_data = np.vstack(all_labeled_data)\n",
    "np.save('dataset.npy', all_labeled_data)\n",
    "print(f\"Labeled data saved to: {'dataset.npy'}, shape: {all_labeled_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc291e95",
   "metadata": {},
   "source": [
    "#### Model training\n",
    "\n",
    "Only a demo for training a model is provided here.\n",
    "\n",
    "Note:\n",
    "The current implementation employs a standard neural network configuration as a foundational reference. Users are encouraged to adapt the network parameters based on their specific problem domains and performance criteria, including the max_poch, the structure of networks, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5aa0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cantera as ct\n",
    "from dfode_kit.dfode_core.train.formation import formation_calculate\n",
    "\n",
    "source_file = 'dataset.npy'\n",
    "time_step = 1e-06\n",
    "output_path = './model.pt'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "labeled_data = np.load(source_file)\n",
    "\n",
    "gas = ct.Solution(mech)\n",
    "n_species = gas.n_species\n",
    "formation_enthalpies = formation_calculate(mech)\n",
    "\n",
    "# Model instantiation\n",
    "demo_model = MLP([2+n_species, 400, 400, 400, 400, n_species-1]).to(device)\n",
    "\n",
    "# Data loading\n",
    "thermochem_states1 = labeled_data[:, 0:2+n_species]\n",
    "thermochem_states2 = labeled_data[:, 2+n_species:]\n",
    "\n",
    "print(thermochem_states1.shape, thermochem_states2.shape)\n",
    "thermochem_states1[:, 2:] = np.clip(thermochem_states1[:, 2:], 0, 1)\n",
    "thermochem_states2[:, 2:] = np.clip(thermochem_states2[:, 2:], 0, 1)\n",
    "\n",
    "features = torch.tensor(np.hstack((thermochem_states1[:, :2], BCT(thermochem_states1[:, 2:]))), dtype=torch.float32).to(device)\n",
    "labels = torch.tensor(BCT(thermochem_states2[:, 2:-1]) - BCT(thermochem_states1[:, 2:-1]), dtype=torch.float32).to(device)\n",
    "\n",
    "features_mean = torch.mean(features, dim=0)\n",
    "features_std = torch.std(features, dim=0)\n",
    "features = (features - features_mean) / features_std\n",
    "\n",
    "labels_mean = torch.mean(labels, dim=0)\n",
    "labels_std = torch.std(labels, dim=0)\n",
    "labels = (labels - labels_mean) / labels_std\n",
    "\n",
    "formation_enthalpies = torch.tensor(formation_enthalpies, dtype=torch.float32).to(device)\n",
    "\n",
    "# Training\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "demo_model.train()  \n",
    "\n",
    "max_epochs = 1500\n",
    "initial_lr = 0.001\n",
    "lr_decay_epoch = 500\n",
    "batch_size = 2000\n",
    "optimizer = torch.optim.Adam(demo_model.parameters(), lr=initial_lr)\n",
    "\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    if epoch > 0 and epoch % lr_decay_epoch == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.1\n",
    "    \n",
    "    # 初始化损失值\n",
    "    total_loss1 = 0\n",
    "    total_loss2 = 0\n",
    "    total_loss3 = 0\n",
    "    total_batches = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(0, len(features), batch_size):\n",
    "        batch_features = features[i:i + batch_size]\n",
    "        batch_labels = labels[i:i + batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = demo_model(batch_features)\n",
    "        loss1 = loss_fn(preds, batch_labels)  \n",
    "\n",
    "        Y_in = ((batch_features[:, 2:-1] * features_std[2:-1] + features_mean[2:-1]) * 0.1 + 1) ** 10\n",
    "        Y_out = (((preds * labels_std + labels_mean) + (batch_features[:, 2:-1] * features_std[2:-1] + features_mean[2:-1])) * 0.1 + 1) ** 10\n",
    "        Y_target = (((batch_labels * labels_std + labels_mean) + (batch_features[:, 2:-1] * features_std[2:-1] + features_mean[2:-1])) * 0.1 + 1) ** 10\n",
    "\n",
    "        loss2 = loss_fn(Y_out.sum(axis=1), Y_in.sum(axis=1))\n",
    "\n",
    "        Y_out_total = torch.cat((Y_out, (1 - Y_out.sum(axis=1)).reshape(Y_out.shape[0], 1)), axis=1)\n",
    "        Y_target_total = torch.cat((Y_target, (1 - Y_target.sum(axis=1)).reshape(Y_target.shape[0], 1)), axis=1)\n",
    "\n",
    "        loss3 = loss_fn((formation_enthalpies * Y_out_total).sum(axis=1), (formation_enthalpies * Y_target_total).sum(axis=1)) / time_step\n",
    "        loss = loss1 + loss2 + loss3 / 1e+13\n",
    "\n",
    "        total_loss1 += loss1.item()\n",
    "        total_loss2 += loss2.item()\n",
    "        total_loss3 += loss3.item()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    total_loss1 /= (len(features) / batch_size)\n",
    "    total_loss2 /= (len(features) / batch_size)\n",
    "    total_loss3 /= (len(features) / batch_size)\n",
    "    total_loss /= (len(features) / batch_size)\n",
    "\n",
    "    print(\"Epoch: {}, Loss1: {:4e}, Loss2: {:4e}, Loss3: {:4e}, Loss: {:4e}\".format(epoch+1, total_loss1, total_loss2, total_loss3, total_loss))\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        'net': demo_model.state_dict(),\n",
    "        'data_in_mean': features_mean.cpu().numpy(),\n",
    "        'data_in_std': features_std.cpu().numpy(),\n",
    "        'data_target_mean': labels_mean.cpu().numpy(),\n",
    "        'data_target_std': labels_std.cpu().numpy(),\n",
    "    },\n",
    "    output_path\n",
    ")\n",
    "\n",
    "# The above is equivalent to the following cli command:\n",
    "# dfode-kit train --mech ../../mechanisms/Burke2012_s9r23.yaml     \\\n",
    "#     --source_file ./dataset.npy     \\\n",
    "#     --output_path ./demo_model.pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "df",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
